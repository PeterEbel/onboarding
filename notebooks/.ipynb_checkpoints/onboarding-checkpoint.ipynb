{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a07e12f-c51a-4c20-a13c-a7f07250212e",
   "metadata": {},
   "source": [
    "# **Programming Spark using Jupyter Notebook**\n",
    "---\n",
    "__Santander Consumer Bank Germany__  \n",
    "__CTO & IT Architecture__  \n",
    "\n",
    "__Version:__ 1.0  \n",
    "__Date:__ 2024-04-03  \n",
    "__Github:__\n",
    "\n",
    "This document is intended to serve as a first introduction to programming Apache Spark using the Python framework PySpark. Basic knowledge of the Python programming language is required. The most important concepts and data structures of Spark, in particular Resilient Distributed Datasets (RDD) and DataFrames, are presented using short example programs. Jupyter Notebook and Microsoft Visual Code are used as development environments. Many of the examples are taken from the [official Spark documentation](https://spark.apache.org/docs/latest/index.html) and some have been slightly modified.\n",
    "\n",
    "## What is Apache Spark?\n",
    "\n",
    "Apache Sparkâ„¢ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.\n",
    "\n",
    "## **How to create a development environment**\n",
    "\n",
    "### **Install Docker**\n",
    "Install Docker following the instructions for your operating system.\n",
    "\n",
    "### Download the jupyter/pyspark-notebook image\n",
    "\n",
    "Once installed download the jupyter/pyspark-notebook image.\n",
    "```\n",
    "docker pull jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "\n",
    "### Create a bash file\n",
    "\n",
    "Create a bash file (e.g. run.sh) with the following content:\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "CONTAINER=$(docker run -d --rm --name my-pyspark -p 8888:8888 -v /home/peter/projects:/home/jovyan/work jupyter/pyspark-notebook)\n",
    "docker cp /home/peter/projects/postgres/lib/postgresql-42.7.0.jar $CONTAINER:/usr/local/spark/jars\n",
    "docker cp /home/peter/projects/iceberg/lib/iceberg-spark-runtime-3.5_2.12-1.4.0.jar $CONTAINER:/usr/local/spark/jars\n",
    "export CONTAINER\n",
    "sleep 5\n",
    "docker exec $CONTAINER jupyter server list\n",
    "```\n",
    "\n",
    "For Windows, create a corresponding Powershell file and adapt the syntax join above. \n",
    "\n",
    "The second line creates a container (with the name \"my-pyspark\") from the downloaded image, maps the Juypter port 8888 so it becomes accessible outside the container under the same port number, and additionally maps the pre-configured home directory inside the container (/home/jovyan/work) to a folder in the file system of your operating system (here: /home/peter/projects). Any filed stored there will appear later inside the container as if it were local. The third and fourth line show how to copy libraries like database drivers into Sparks library folder inside the container (e.g. to read from a Postgres database within your Spark program).\n",
    "\n",
    "### Open the Jupyter Notebook in your browser \n",
    "\n",
    "Open your preferred browser and enter the following as URL:\n",
    "\n",
    "```\n",
    "localhost:8888/tree?token=0f9541f307a73fcd220474bfd24d2476ea145d58d165ad1b\n",
    "```\n",
    "The token (here: 0f9541f307a73fcd220474bfd24d2476ea145d58d165ad1b) will be different each time you start Jupyter Notebook. The token you need for the current Juypter session is shown on the screen when the run script terminates. Look for \"token=\".\n",
    "\n",
    "```\n",
    "Currently running servers:\n",
    "http://cc03a1a1513f:8888/?token=2ea951ec0dc87115a4f40a7b21f1a7b823ce3379a14f94fb\n",
    "```\n",
    "\n",
    "## How to run Jupyter Notebook and PySpark in Anaconda\n",
    "\n",
    "### Install Anaconda or Minoconda\n",
    "\n",
    "Install Anaconda or Miniconda following the instructions for your operating system.\n",
    "\n",
    "### Create a virtual environment\n",
    "\n",
    "Create a file env.yml with the following content in your working directory. Replace the name \"Onboarding\" with the name of your project.\n",
    "\n",
    "```\n",
    "name: onboarding\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.10\n",
    "  - pyspark=3.5\n",
    "  - pypandoc=1.12\n",
    "  - pytest=7.4.3\n",
    "  - pylint=3.0.3\n",
    "  - findspark=2.0.1\n",
    "  - jupyter=1.0.0\n",
    "  - pandas=2.2.1\n",
    "  - numpy=1.26.4\n",
    "  - openpyxl=3.1.2\n",
    "  ```\n",
    "  Then run\n",
    "\n",
    "  ```\n",
    "  conda env create -f env.yml\n",
    "  ```\n",
    "\n",
    "Conda will download all the required packages and take care of all dependencies. This may take a couple minutes. Once ready activate the new environment with\n",
    "\n",
    "```\n",
    "conda activate onboarding\n",
    "```\n",
    "\n",
    "To start Jupyter Notebook enter\n",
    "\n",
    "```\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de36aad-c2c8-40c4-85af-20671df0d43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7b836cc65350>\n",
      "<SparkContext master=local[*] appName=Introduction to Spark>\n"
     ]
    }
   ],
   "source": [
    "# SparkSession und SparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Introduction to Spark\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d432e43-1515-4f3d-aa18-e0dbde86244c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# RDDs and DataFrames\n",
    "\n",
    "[RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)  \n",
    "[SQL Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)  \n",
    "[Getting Started - PySpark](https://spark.apache.org/docs/latest/api/python/getting_started/index.html)\n",
    "\n",
    "## What are RDDs?\n",
    "A __RDD__, or Resilient Distributed Dataset, is a fundamental data structure in Apache Spark. RDDs are designed to provide an efficient and fault-tolerant way to distribute and process data across a cluster of computers. They have several key characteristics:\n",
    "\n",
    "* __Resilient:__ RDDs are \"resilient\" because they can recover from node failures. If a node in the cluster fails, the data and the operations applied to it can be reconstructed on another node, ensuring fault tolerance.\n",
    "* __Distributed:__ RDDs are distributed across multiple nodes in a cluster, allowing for parallel processing of data. This distribution is a fundamental feature of Spark, making it suitable for handling large datasets.\n",
    "* __Immutable:__ Once created, RDDs are immutable, meaning their data cannot be changed. Instead, transformations applied to an RDD result in the creation of new RDDs, which helps maintain data consistency and enables lineage information for fault recovery.\n",
    "* __In-Memory:__ RDDs are typically stored in memory, which makes them faster to access and process compared to reading and writing to disk. This in-memory storage is one of the key performance benefits of Spark.\n",
    "* __Lazy execution:__ When Spark transforms data, it does not immediately compute the transformation but plans how to compute later. When actions such as collect() are explicitly called, the computation starts.\n",
    "\n",
    "Common operations on RDDs include mapping, filtering, reducing, and joining data.\n",
    "\n",
    "While RDDs were the primary data abstraction in earlier versions of Spark, more recent versions introduced __DataFrames__ and Datasets, which provide higher-level abstractions and optimizations for structured data processing. Nevertheless, RDDs still play a crucial role in Spark, particularly when you need fine-grained control over data and operations or when working with unstructured data.\n",
    "\n",
    "## What are DataFrames?\n",
    "Spark __DataFrames__ are a distributed collection of data organized into named columns, much like a table in a relational database or a data frame in R or Python. They are a higher-level abstraction built on top of Resilient Distributed Datasets (RDDs) in Apache Spark, designed to provide a more structured and optimized way to work with data. DataFrames were introduced in Spark 2.0 to address the limitations of RDDs, particularly when dealing with structured and semi-structured data.\n",
    "\n",
    "Key features and characteristics of Spark DataFrames include:\n",
    "* __Schema:__ DataFrames have a well-defined schema that describes the structure of the data, including column names and data types. This schema information helps Spark optimize query execution and provides type safety.\n",
    "* __Performance:__ DataFrames are designed for optimized performance. Spark can leverage the schema information to perform predicate pushdown, column pruning, and other query optimizations to speed up data processing.\n",
    "* __Language support:__ DataFrames are available in multiple programming languages, including Scala, Java, Python, and R. This means you can work with DataFrames in a language you're comfortable with.\n",
    "* __Interoperability:__ DataFrames can seamlessly interoperate with RDDs, allowing you to use the right data abstraction for your specific use case.\n",
    "* __Catalyst query optimizer:__ Spark DataFrames use the Catalyst query optimizer, which is a powerful tool for optimizing query plans. Catalyst can apply a wide range of optimizations, making queries more efficient.\n",
    "* __Tungsten execution engine:__ DataFrames also use the Tungsten execution engine, which is designed for code generation and in-memory processing. This engine further enhances performance.\n",
    "* __Data sources:__ DataFrames can read and write data from various sources, including Parquet, Avro, ORC, JSON, CSV, and more, making it easy to work with a variety of data formats.\n",
    "* __SQL support:__ Spark DataFrames support SQL queries, enabling you to write SQL-like statements for data manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a37bcb-0f57-4913-8ece-d4be55306d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDDs\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDDs\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd_numbers = sc.parallelize(range(1, 11))\n",
    "# altenatively: rdd_numbers = spark.sparkContext.parallelize(range(1, 11))\n",
    "\n",
    "list_numbers = rdd_numbers.collect()\n",
    "# print(list_numbers[5])\n",
    "\n",
    "for n in list_numbers:\n",
    "   print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58be4e-0e04-48ad-b7a4-48a79dfc9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map and Reduce\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Map and Reduce\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd_squared_numbers = sc.parallelize(range(1, 11)).map(lambda n: n**2)\n",
    "# altenatively: rdd_numbers = spark.sparkContext.parallelize(range(1, 11))\n",
    "\n",
    "list_numbers = rdd_squared_numbers.collect()\n",
    "\n",
    "print(\"The first 10 square numbers are:\")\n",
    "print()\n",
    "\n",
    "for n in list_numbers:\n",
    "   print(n)\n",
    "print()    \n",
    "\n",
    "sum = sc.parallelize(range(1, 11)).map(lambda n: n**2).reduce(add)\n",
    "print(\"The Sum of the first 10 square numbers is {summe}\".format(summe = sum))\n",
    "print()\n",
    "\n",
    "rdd = sc.parallelize([(\"Peter\", 10), (\"Thomas\", 10), (\"Peter\", 20), (\"Thomas\", 30)])\n",
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1697196-b538-45b6-aed6-c4f0c6484ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Filter\").getOrCreate() \n",
    "\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382d9b1-bed0-4a82-814a-076dd9eb7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing functions as parameters\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder.appName(\"Passing functions as parameters\").getOrCreate() \n",
    "\n",
    "def myFunction(s):\n",
    "    words = s.split(\" \")\n",
    "    return len(words)\n",
    "\n",
    "rdd_words = spark.sparkContext.textFile(\"/home/jovyan/work/spark/data/words.txt\").map(myFunction)\n",
    "number_of_words_in_file = rdd_words.collect()[0]\n",
    "\n",
    "print(\"The file contains {count} words.\".format(count = number_of_words_in_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45a5f8-afe1-4908-a78f-a76dfa04e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pi\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae107f-5c29-4e5d-813b-79d610dd416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an RDD from a file\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder.appName(\"RDD from a file\").getOrCreate() \n",
    "\n",
    "rdd_data = spark.sparkContext.textFile(\"/home/jovyan/work/spark/data/data.txt\")\n",
    "print(rdd_data.collect())\n",
    "lineLengths = rdd_data.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a821cd-63f2-4145-8607-825dcef4a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small text files\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder.appName(\"Small text files\").getOrCreate() \n",
    "\n",
    "# Load a text file and convert each line to a Row\n",
    "rdd_textfiles = spark.sparkContext.wholeTextFiles(\"/home/jovyan/work/spark/data/small-text-files\")\n",
    "\n",
    "list_textfiles = rdd_textfiles.sortByKey().collect()\n",
    "\n",
    "for tf in list_textfiles:\n",
    "    print(tf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6b60f-9f8b-4663-93dc-818840ac6fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD finger exercises\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder.appName(\"RDDs\").getOrCreate() \n",
    "\n",
    "# Create RDD from parallelize    \n",
    "list_person = [(\"Peter\", 60), (\"Thomas\", 59), (\"Michael\", 54), (\"Anabel\", 50)]\n",
    "rdd_person = spark.sparkContext.parallelize(list_person)\n",
    "rdd_person.setName('Personen')\n",
    "\n",
    "print(\"The RDD {rdd_name} contains {anzahl} records.\".format(rdd_name = rdd_person.name(), anzahl = rdd_person.count()))\n",
    "print()\n",
    "\n",
    "for p in rdd_person.collect():\n",
    "  # Print name only\n",
    "    print(p[0])\n",
    "print()\n",
    "    \n",
    "print(\"First record before sorting: {ds}\".format(ds = rdd_person.first()))\n",
    "rdd_person_sorted = rdd_person.sortByKey()\n",
    "print(\"First record after sorting:  {ds}\".format(ds = rdd_person_sorted.first()))\n",
    "print()\n",
    "\n",
    "# Filter \n",
    "rdd_numbers = spark.sparkContext.parallelize(range(1, 10 + 1))\n",
    "print(rdd_numbers.collect())\n",
    "print(rdd_numbers.filter(lambda x: x % 2 == 0).collect())\n",
    "print()\n",
    "\n",
    "# Cartesian product \n",
    "rdd1 = spark.sparkContext.parallelize(range(1, 10 + 1))\n",
    "rdd2 = spark.sparkContext.parallelize(range(1, 10 + 1))\n",
    "print(rdd1.cartesian(rdd2).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb231c93-5247-4ea3-8d19-4895b63a6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame from a list of tuples\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes\").getOrCreate()\n",
    "\n",
    "list_of_tuples = [('Peter', 60, 'M'), ('Thomas', 59, 'M'), ('Michael', 54, 'M'), ('Anabel', 50, 'F')]\n",
    "spark.createDataFrame(list_of_tuples).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf107e-2354-4eab-a067-e20c10b7cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame from a list of tuples specifying column names\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes\").getOrCreate()\n",
    "\n",
    "list_of_tuples = [('Peter', 60, 'M'), ('Thomas', 59, 'M'), ('Michael', 54, 'M'), ('Anabel', 50, 'F')]\n",
    "spark.createDataFrame(list_of_tuples, ['Name', 'Age', 'Gender']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f04d6c-7e91-45c8-a36a-00db60c16301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame from a list of dictionaries (key-value-pairs)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes\").getOrCreate()\n",
    "\n",
    "list_of_dictionaries = [{'Name': 'Peter', 'Age':60, 'Gender': 'M'}, {'Name': 'Thomas', 'Age':59, 'Gender': 'M'}, {'Name': 'Michael', 'Age':54, 'Gender': 'M'}, {'Name': 'Anabel', 'Age':50, 'Gender': 'F'}]\n",
    "df = spark.createDataFrame(list_of_dictionaries).select('Name', 'Age', 'Gender')\n",
    "# df.show(2)\n",
    "# df.show(vertical = True)\n",
    "# df.show(2, vertical = True)\n",
    "df.filter(df.Gender != 'F').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e96294-fa18-45ee-8d1d-7d685d9b5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with a schema inferred from the data\n",
    "\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes 2\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "\n",
    "print(df) # Schema is inferred\n",
    "print()\n",
    "\n",
    "for r in df.collect():\n",
    "   print(r[0], r[1], r[2], r[3], r[4] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47d00b-5003-4f86-bf88-b12e8193973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with an explicit schema\n",
    "\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes 3\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "\n",
    "# df.show()\n",
    "# df.show(1, vertical = True)\n",
    "\n",
    "# df.printSchema()\n",
    "# df.columns\n",
    "# df.select(\"a\", \"b\", \"c\").describe().show()\n",
    "# print(type(df.a))\n",
    "# df.select(df.c).show()\n",
    "# df.collect()\n",
    "# df.take(2)\n",
    "# df.filter(df.a == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c466f-82c5-4732-b4c8-9621bdc67a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame aggregations\n",
    "\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes 4\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "\n",
    "df.show()\n",
    "df.groupby('color').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f0204-44f3-4155-a51f-9ddadb318a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames: Processing data in CSV files \n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSV\").getOrCreate()\n",
    "\n",
    "df_customers = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .load(\"/home/jovyan/work/spark/data/customers.csv\")\n",
    "\n",
    "# number of records in a dataframe\n",
    "# print(df_customers.count())\n",
    "\n",
    "# show first 10 records of a dataframe\n",
    "# df_customers.limit(10).show()\n",
    "\n",
    "# show selected columns only\n",
    "# df_customers.select('last_name', 'first_name', 'gender_code').limit(10).show()\n",
    "\n",
    "# filter by gender code\n",
    "# df_customers.select('last_name', 'first_name', 'gender_code').filter(\"gender_code == 'F'\").limit(10).show()\n",
    "\n",
    "# write file with female entries only\n",
    "df_female_customers = df_customers.filter(\"gender_code == 'F'\")\n",
    "df_female_customers.limit(10).show()\n",
    "df_female_customers.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"|\").save(\"/home/jovyan/work/spark/data/female_customers.csv\")\n",
    "df2 = spark.read.format(\"csv\").option(\"inferSchema\", \"false\").option(\"header\", \"false\").option(\"sep\", \"|\").load(\"/home/jovyan/work/spark/data/female_customers.csv\")\n",
    "df2.limit(10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d01a0-35a9-4f1a-a82b-f24047faf15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames: Playing around with JSON files\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Read a JSON File\").getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"multiline\",\"true\").json(\"/home/jovyan/work/spark/data/customers.json\")\n",
    "df.printSchema()\n",
    "\n",
    "# Display some columns of the firts 10 DataFrame records\n",
    "df.select('last_name', 'first_name').show(10)\n",
    "\n",
    "# Display females only\n",
    "df.select('last_name', 'first_name', 'gender_code').filter(df['gender_code'] == 'F').show(10)\n",
    "\n",
    "# Group by gender\n",
    "df.groupBy(\"gender_code\").count().show()\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM customers where first_name =='Peter'\")\n",
    "sqlDF.select('first_name', 'last_name', 'birth_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79f95d-6a77-400d-b62b-178f958c3a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregations - grouping. average, min, max\n",
    "\n",
    "# import non-standard functions\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes and aggretations\").getOrCreate()\n",
    "\n",
    "# create a dataframe with named columns from an array of tuples (corresponds to a table with 2 columns and 6 rows) \n",
    "person_df = spark.createDataFrame([(\"Peter\", 20), (\"Thomas\", 31), (\"Michael\", 30), (\"Anabel\", 35), (\"Elke\", 25), (\"Peter\", 58)], [\"name\", \"age\"])\n",
    "\n",
    "# group person with the same name, aggregate over ages and calculate avg age\n",
    "avg_age_df = person_df.groupBy(\"name\").agg(avg(\"age\"))\n",
    "\n",
    "# show results\n",
    "person_df.show()\n",
    "avg_age_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b710a5e2-7500-4fe7-b3f8-adbf45cb45cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing around with RDDs and DataFrames\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDDs and DataFrames\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row\n",
    "rdd_customers_raw = sc.textFile(\"/home/jovyan/work/spark/data/customers.csv\")\n",
    "rdd_balances_raw = sc.textFile(\"/home/jovyan/work/spark/data/balances.csv\")\n",
    "\n",
    "rdd_customer_attributes = rdd_customers_raw.map(lambda l: l.split(\"|\"))\n",
    "rdd_balances_attributes = rdd_balances_raw.map(lambda l: l.split(\"|\"))\n",
    "\n",
    "rdd_customers = rdd_customer_attributes.map(lambda a: Row(record_number=a[0], \\\n",
    "                                         entity_id=a[1], \\\n",
    "                                         customer_number=a[2], \\\n",
    "                                         valid_from_date=a[3], \\\n",
    "                                         valid_to_date=a[4], \\\n",
    "                                         gender_code=a[5], \\\n",
    "                                         last_name=a[6], \\\n",
    "                                         first_name=a[7], \\\n",
    "                                         birth_date=a[8], \\\n",
    "                                         country_code=a[9], \\\n",
    "                                         postal_code=a[10], \\\n",
    "                                         city=a[11], \\\n",
    "                                         street=a[12], \\\n",
    "                                         data_date_part=a[13]))\n",
    "\n",
    "rdd_balances = rdd_balances_attributes.map(lambda a: Row(record_number=a[0], \\\n",
    "                                         entity_id=a[1], \\\n",
    "                                         customer_number=a[2], \\\n",
    "                                         instalment_amount=a[3], \\\n",
    "                                         term=a[4], \\\n",
    "                                         debt_amount=a[5], \\\n",
    "                                         data_date_part=a[6]))\n",
    "\n",
    "# Infer the schemas and register the DataFrames as a tables\n",
    "df_customers = spark.createDataFrame(rdd_customers)\n",
    "df_customers.createOrReplaceTempView(\"customers\")\n",
    "df_balances = spark.createDataFrame(rdd_balances)\n",
    "df_balances.createOrReplaceTempView(\"balances\")\n",
    "\n",
    "# Find balance records that have no customer\n",
    "df_balances_without_customer = spark.sql(\"SELECT balances.* FROM balances LEFT JOIN customers ON balances.customer_number = customers.customer_number WHERE customers.customer_number IS NULL;\")\n",
    "df_balances_without_customer.show()\n",
    "\n",
    "# females = spark.sql(\"SELECT last_name, first_name, gender_code FROM customers WHERE gender_code='F' LIMIT 10\")\n",
    "# female_names = females.rdd.map(lambda f: \"Name: \" + f.first_name).collect()\n",
    "\n",
    "# for name in female_names:\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f000af6-2d67-4be6-914f-ff13b5da764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Iceberg\n",
    "\n",
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"Iceberg\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/home/jovyan/work/iceberg/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_customers = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"/home/jovyan/work/iceberg/data/customers.csv\")\n",
    "\n",
    "# df_customers.select('last_name', 'first_name', 'gender_code', 'data_date_part').filter(\"gender_code = 'F'\").createOrReplaceTempView(\"temp_view_customers\")\n",
    "df_customers.select('*').createOrReplaceTempView(\"temp_view_customers\")\n",
    "spark.sql(\"CREATE or REPLACE TABLE local.db.customers USING iceberg AS SELECT * FROM temp_view_customers\")\n",
    "df = spark.sql(\"SELECT last_name, first_name, data_date_part FROM local.db.customers\")\n",
    "df.show()\n",
    "# df = spark.sql(\"UPDATE local.db.customers SET data_date_part = '2024-01-01' WHERE data_date_part = '2024-02-01' AND first_name = 'Re'\")\n",
    "# df = spark.sql(\"SELECT last_name, first_name, data_date_part FROM local.db.customers WHERE first_name = 'Rebecca'\")\n",
    "# df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9430b6-0644-4584-a372-7df0e872d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Iceberg Update\n",
    "\n",
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"Iceberg\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/home/jovyan/work/iceberg/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_customers = spark.sql(\"SELECT last_name, first_name, data_date_part FROM local.db.customers\")\n",
    "df_customers.show()\n",
    "df = spark.sql(\"UPDATE local.db.customers SET first_name = 'Stefanie' WHERE first_name = 'Stefani'\")\n",
    "df_customers = spark.sql(\"SELECT last_name, first_name, data_date_part FROM local.db.customers\")\n",
    "df_customers.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19f83e-0051-49ad-b6f1-eb727bc3d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Iceberg Delete\n",
    "\n",
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"Iceberg\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/home/jovyan/work/iceberg/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_customers = spark.sql(\"SELECT last_name, first_name, data_date_part FROM local.db.customers\")\n",
    "df_customers.show()\n",
    "df = spark.sql(\"DELETE FROM local.db.customers WHERE first_name = 'Stefanie'\")\n",
    "df_customers = spark.sql(\"SELECT last_name, first_name, data_date_part FROM local.db.customers\")\n",
    "df_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c19a7e-42f2-4781-9507-c232d2b16a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a function to read from a MariaDB and from PostgreSQL database table\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Databases\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark = SparkSession \\\n",
    "#    .builder \\\n",
    "#    .appName(\"Databases\") \\\n",
    "# location of the drivers in non-Docker envs\n",
    "# in Docker driver must be copied into container to /usr/local/spark/jars with docker cp\n",
    "#    .config(\"spark.jars\", \"/home/peter/projects/spark/postgresql-42.7.0.jar\") \\ \n",
    "#    .getOrCreate()\n",
    "\n",
    "def show_customers(spark: SparkSession, database) -> None:\n",
    "    if (database.lower() == \"mariadb\"):\n",
    "        df_customers = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:mysql://localhost:3306/shop?permitMysqlScheme\") \\\n",
    "            .option(\"driver\", \"org.mariadb.jdbc.Driver\") \\\n",
    "            .option(\"dbtable\", \"customers\") \\\n",
    "            .option(\"user\", \"postgres\") \\\n",
    "            .option(\"password\", \"guiltyspark\") \\\n",
    "            .load()\n",
    "        df_customers.select('last_name', 'first_name', 'birth_date').show(10)\n",
    "    elif (database.lower() == \"postgresql\"):\n",
    "        df_customers = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:postgresql://172.17.0.2:5432/postgres\") \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .option(\"dbtable\", \"customers\") \\\n",
    "            .option(\"user\", \"postgres\") \\\n",
    "            .option(\"password\", \"guiltyspark\") \\\n",
    "            .load()\n",
    "        df_customers.select('last_name', 'first_name', 'birth_date').show(100)\n",
    "    else:\n",
    "        print(\"Unbekannter Datenbanktyp\")\n",
    "\n",
    "show_customers(spark, \"postgresql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b123ce14-b78e-4bb6-93df-94142db71fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and read Parquet files\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Write and read a Parquet file\").getOrCreate()\n",
    "\n",
    "customers_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \"|\").load(\"/home/jovyan/work/spark/data/customers.csv\")\n",
    "customers_df.write.mode(\"overwrite\").parquet(\"/home/jovyan/work/spark/data/customers.parquet\")\n",
    "parquet_df = spark.read.parquet(\"/home/jovyan/work/spark/data/customers.parquet\")\n",
    "parquet_df.createOrReplaceTempView(\"parquet_file\")\n",
    "males_df = spark.sql(\"SELECT last_name, first_name, gender_code FROM parquet_file WHERE gender_code = 'M'\")\n",
    "males_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5ca7f-e05a-426b-852b-2f2e666585a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 12345) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5c145-850d-4863-b5e5-3f307df61dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and read to/from Redis\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Databases\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark-3.5.0/jars/mariadb-java-client-3.2.0.jar\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark-3.5.0/jars/spark-redis_2.12-3.1.0.jar\") \\\n",
    "    .config(\"spark.redis.host\", \"localhost\") \\\n",
    "    .config(\"spark.redis.port\", \"6379\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd_customers_raw = sc.textFile(\"/home/peter/Projects/spark/data/customers.csv\")\n",
    "# one way to skip a header...\n",
    "rdd_customers_headers = rdd_customers_raw.first()\n",
    "rdd_customers = rdd_customers_raw.filter(lambda x: x != rdd_customers_headers) \n",
    "rdd_customers.collect()\n",
    "rdd_customer_attributes = rdd_customers.map(lambda l: l.split(\"|\"))\n",
    "\n",
    "schema = StructType([StructField(\"record_number\", StringType(), False),          \n",
    "                     StructField(\"entity_id\", StringType(), False),\n",
    "                     StructField(\"customer_number\", StringType(), False),\n",
    "                     StructField(\"valid_from_date\", StringType(), False),\n",
    "                     StructField(\"valid_to_date\", StringType(), False),\n",
    "                     StructField(\"gender_code\", StringType(), False),\n",
    "                     StructField(\"last_name\", StringType(), False),\n",
    "                     StructField(\"first_name\", StringType(), False),\n",
    "                     StructField(\"birth_date\", StringType(), False),\n",
    "                     StructField(\"country_code\", StringType(), False),\n",
    "                     StructField(\"postal_code\", StringType(), False),\n",
    "                     StructField(\"city\", StringType(), False),\n",
    "                     StructField(\"street\", StringType(), False),\n",
    "                     StructField(\"data_date_part\", StringType(), False)])\n",
    "\n",
    "df_customers = spark.createDataFrame(rdd_customer_attributes, schema)\n",
    "\n",
    "df_customers.write\\\n",
    "  .format(\"org.apache.spark.sql.redis\")\\\n",
    "  .option(\"table\", \"customers\")\\\n",
    "  .option(\"key.column\", \"record_number\")\\\n",
    "  .mode(\"Overwrite\")\\\n",
    "  .save()\n",
    "\n",
    "df_redis_customers = spark.read.format(\"org.apache.spark.sql.redis\").option(\"table\", \"customers\").option(\"key.column\", \"name\").load()\n",
    "df_redis_customers.select('last_name', 'first_name').filter(df_redis_customers.gender_code == 'M').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b849b4c5-04bc-43f2-9d52-731b1edd4b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/03 09:11:38 WARN Utils: Your hostname, lenovo-xubuntu resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface ens33)\n",
      "24/04/03 09:11:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/03 09:11:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------------+--------+-----------+--------+--------+------------------+-------+----+\n",
      "|Nummer|          Bundesland|LÃ¤nderkÃ¼rzel|  FlÃ¤che|BevÃ¶lkerung|MÃ¤nnlich|Weiblich|BevÃ¶lkerungsdichte|    BIP|Jahr|\n",
      "+------+--------------------+------------+--------+-----------+--------+--------+------------------+-------+----+\n",
      "|    01|  Schleswig-Holstein|          SH|15804.30|    2953270| 1443269| 1510001|               187|118.680|2023|\n",
      "|    02|             Hamburg|          HH|  755.09|    1892122|  925616|  966506|              2506|150.575|2023|\n",
      "|    03|       Niedersachsen|          NI|47709.90|    8140242| 4009822| 4130420|               171|363.109|2023|\n",
      "|    04|              Bremen|          HB|  419.61|     684864|  338233|  346631|              1632| 39.252|2023|\n",
      "|    05| Nordrhein-Westfalen|          NW|34112.72|   18139116| 8890200| 9248916|               532|839.084|2023|\n",
      "|    06|              Hessen|          HE|21115.62|    6391360| 3151158| 3240202|               303|351.139|2023|\n",
      "|    07|     Rheinland-Pfalz|          RP|19857.97|    4159150| 2054254| 2104896|               209|174.249|2023|\n",
      "|    08|   Baden-WÃ¼rttemberg|          BW|35747.85|   11280257| 5595424| 5684833|               316|615.071|2023|\n",
      "|    09|              Bayern|          BY|70541.58|   13369393| 6620203| 6749190|               190|768.469|2023|\n",
      "|    10|            Saarland|          SL| 2571.52|     992666|  487101|  505565|               386| 41.348|2023|\n",
      "|    11|              Berlin|          BE|  891.12|    3755251| 1843196| 1912055|              4214|193.219|2023|\n",
      "|    12|         Brandenburg|          BB|29654.38|    2573135| 1265151| 1307984|                87| 97.477|2023|\n",
      "|    13|Mecklenburg-Vorpo...|          MV|23294.90|    1628378|  799695|  828683|                70| 59.217|2023|\n",
      "|    14|             Sachsen|          SN|18449.86|    4086152| 2010537| 2075615|               221|155.982|2023|\n",
      "|    15|      Sachsen-Anhalt|          ST|20467.20|    2186643| 1073711| 1112932|               107| 78.380|2023|\n",
      "|    16|           ThÃ¼ringen|          TH|16202.37|    2126846| 1051482| 1075364|               131| 75.909|2023|\n",
      "+------+--------------------+------------+--------+-----------+--------+--------+------------------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read a CSV file and convert it to Parquet\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSV-to-Parquet converter\").getOrCreate()\n",
    "\n",
    "states_csv_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \"|\").load(\"/home/peter/projects/onboarding/data/csv/states.csv\")\n",
    "states_csv_df.write.mode(\"overwrite\").parquet(\"/home/peter/projects/onboarding/data/parquet/states.parquet\")\n",
    "states_parquet_df = spark.read.parquet(\"/home/peter/projects/onboarding/data/parquet/states.parquet\")\n",
    "states_parquet_df.createOrReplaceTempView(\"states\")\n",
    "states_sql_df = spark.sql(\"SELECT * FROM states\")\n",
    "states_sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbec49-30a6-475c-abcf-904e84ec598c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
