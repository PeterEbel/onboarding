{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a07e12f-c51a-4c20-a13c-a7f07250212e",
   "metadata": {},
   "source": [
    "## **Programming Spark using Jupyter Notebook**\n",
    "---\n",
    "__Santander Consumer Bank Germany__  \n",
    "__CTO & IT Architecture__  \n",
    "\n",
    "__Version:__ 1.0  \n",
    "__Date:__ 2024-04-03  \n",
    "__Github:__\n",
    "\n",
    "Jupyter Notebook and Microsoft Visual Code are used as development environments. Many of the examples are taken from the [official Spark documentation](https://spark.apache.org/docs/latest/index.html) and some have been slightly modified.\n",
    "\n",
    "### **How to create a development environment using Docker**\n",
    "---\n",
    "#### **Install Docker**\n",
    "Install Docker following the instructions for your operating system.\n",
    "\n",
    "#### **Download the jupyter/pyspark-notebook image**\n",
    "\n",
    "Once installed download the jupyter/pyspark-notebook image.\n",
    "```\n",
    "docker pull jupyter/pyspark-notebook\n",
    "```\n",
    "### **Create a bash file**\n",
    "\n",
    "Create a bash file (e.g. run.sh) with the following content:\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "CONTAINER=$(docker run -d --rm --name my-pyspark -p 8888:8888 -v /home/peter/projects:/home/jovyan/work jupyter/pyspark-notebook)\n",
    "docker cp /home/peter/projects/postgres/lib/postgresql-42.7.0.jar $CONTAINER:/usr/local/spark/jars\n",
    "docker cp /home/peter/projects/iceberg/lib/iceberg-spark-runtime-3.5_2.12-1.4.0.jar $CONTAINER:/usr/local/spark/jars\n",
    "export CONTAINER\n",
    "sleep 5\n",
    "docker exec $CONTAINER jupyter server list\n",
    "```\n",
    "\n",
    "For Windows, create a corresponding Powershell file and adapt the syntax join above. \n",
    "\n",
    "The second line creates a container (with the name \"my-pyspark\") from the downloaded image, maps the Juypter port 8888 so it becomes accessible outside the container under the same port number, and additionally maps the pre-configured home directory inside the container (/home/jovyan/work) to a folder in the file system of your operating system (here: /home/peter/projects). Any filed stored there will appear later inside the container as if it were local. The third and fourth line show how to copy libraries like database drivers into Sparks library folder inside the container (e.g. to read from a Postgres database within your Spark program).\n",
    "\n",
    "### **Open the Jupyter Notebook in your browser**\n",
    "\n",
    "Open your preferred browser and enter the following as URL:\n",
    "\n",
    "```\n",
    "localhost:8888/tree?token=0f9541f307a73fcd220474bfd24d2476ea145d58d165ad1b\n",
    "```\n",
    "The token (here: 0f9541f307a73fcd220474bfd24d2476ea145d58d165ad1b) will be different each time you start Jupyter Notebook. The token you need for the current Juypter session is shown on the screen when the run script terminates. Look for \"token=\".\n",
    "\n",
    "```\n",
    "Currently running servers:\n",
    "http://cc03a1a1513f:8888/?token=2ea951ec0dc87115a4f40a7b21f1a7b823ce3379a14f94fb\n",
    "```\n",
    "\n",
    "## **How to create a development environment using Anaconda**\n",
    "---\n",
    "### **Install Anaconda or Minoconda**\n",
    "\n",
    "Install Anaconda or Miniconda following the instructions for your operating system.\n",
    "\n",
    "### **Create a virtual environment**\n",
    "\n",
    "Create a file env.yml with the following content in your working directory. Replace the name \"Onboarding\" with the name of your project.\n",
    "\n",
    "```\n",
    "name: onboarding\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.10\n",
    "  - pyspark=3.5\n",
    "  - pypandoc=1.12\n",
    "  - pytest=7.4.3\n",
    "  - pylint=3.0.3\n",
    "  - findspark=2.0.1\n",
    "  - jupyter=1.0.0\n",
    "  - pandas=2.2.1\n",
    "  - numpy=1.26.4\n",
    "  - openpyxl=3.1.2\n",
    "  ```\n",
    "  Then run\n",
    "\n",
    "  ```\n",
    "  conda env create -f env.yml\n",
    "  ```\n",
    "\n",
    "Conda will download all the required packages and take care of all dependencies. This may take a couple minutes. Once ready activate the new environment with\n",
    "\n",
    "```\n",
    "conda activate onboarding\n",
    "```\n",
    "\n",
    "To start Jupyter Notebook enter\n",
    "\n",
    "```\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f99e594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/local/spark/python', '/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip', '/home/jovyan', '/opt/conda/lib/python311.zip', '/opt/conda/lib/python3.11', '/opt/conda/lib/python3.11/lib-dynload', '', '/opt/conda/lib/python3.11/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e90b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de36aad-c2c8-40c4-85af-20671df0d43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x78c0b7ac9480>\n",
      "<SparkContext master=local[*] appName=Databases>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/04 15:07:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# SparkSession und SparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Introduction to Spark\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3c19a7e-42f2-4781-9507-c232d2b16a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/04 15:11:12 WARN Utils: Your hostname, lenovo-xubuntu resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface ens33)\n",
      "24/04/04 15:11:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/04 15:11:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+----------+\n",
      "|   last_name|   first_name|birth_date|\n",
      "+------------+-------------+----------+\n",
      "|    Schottin|         Reni|2001-01-17|\n",
      "|        Drub|     Annelene|1962-08-17|\n",
      "|      Ladeck|  Wolf-Dieter|1946-02-25|\n",
      "|   Cichorius|       Marlen|1957-05-29|\n",
      "|       Zobel|        Cemil|1946-01-18|\n",
      "|  Neureuther|        Oscar|1997-10-08|\n",
      "|    Stiebitz|      Rebecca|1991-07-04|\n",
      "|   Eberhardt|      Mariola|1947-02-06|\n",
      "|    Weinhold|        Thilo|1995-07-13|\n",
      "|    Hartmann|     Mohammed|1948-02-18|\n",
      "|        Bähr|     Maurizio|1989-04-23|\n",
      "|      Thanel|    Katharina|1953-10-22|\n",
      "|      Heuser|       Detlev|1959-03-05|\n",
      "|        Graf|        Ester|1976-04-13|\n",
      "|     Hettner|      Diether|1954-10-27|\n",
      "|        Gute|     Christel|2004-06-19|\n",
      "|    Barkholz|      Swantje|2002-06-30|\n",
      "|        Kade|      Erdmute|1971-05-14|\n",
      "|      Albers|         Jiri|1948-05-23|\n",
      "|     Schacht|        Julie|1983-09-21|\n",
      "|    Eckbauer|         Ines|1958-07-05|\n",
      "|     Hornich|     Brunhild|1999-05-31|\n",
      "|       Klemt|      Klemens|1953-07-06|\n",
      "|    Kitzmann|   Ehrenfried|1968-05-22|\n",
      "|     Meister|        Armin|1965-03-14|\n",
      "|       Heinz|      Rebecca|1993-10-18|\n",
      "|       Stroh|       Helene|1974-05-03|\n",
      "|     Zorbach|       Irmela|1999-11-23|\n",
      "|     Radisch|       Marica|1948-03-22|\n",
      "|     Stiffel|         Meta|1965-11-07|\n",
      "|       Riehl|     Reinhart|1952-07-19|\n",
      "|        Ring|     Theresia|1994-08-01|\n",
      "|    Buchholz|       Holger|1946-11-25|\n",
      "|    Pergande|       Marlen|1951-04-04|\n",
      "|        Trub|     Ljudmila|1969-06-11|\n",
      "|       Kensy|        Heide|1962-01-01|\n",
      "|       Gertz|         Arzu|1957-06-01|\n",
      "|     Liebelt|      Mareike|2005-03-23|\n",
      "|      Söding|        Marek|2005-02-13|\n",
      "|      Hethur|     Diethard|1977-03-16|\n",
      "|   Reichmann|     Reinhild|1953-10-05|\n",
      "|        Döhn|    Mechthild|1989-07-23|\n",
      "|     Winkler|    Kunigunde|1975-12-06|\n",
      "|        Pohl|      Jolanta|1995-07-10|\n",
      "|       Höfig|        Thies|1988-03-29|\n",
      "|    Hoffmann|       Gunter|1964-10-11|\n",
      "|   Striebitz|       Cosima|1955-01-24|\n",
      "| Ditschlerin|         Olga|1970-11-14|\n",
      "|    Schmiedt|    Catharina|1972-12-04|\n",
      "|  Neureuther|       Leonid|2004-06-24|\n",
      "|       Beyer|      Stefano|2005-10-05|\n",
      "|Klingelhöfer|       Frieda|1949-06-06|\n",
      "|      Biggen|      Michail|1967-08-15|\n",
      "|  Vollbrecht|    Angelique|2000-05-02|\n",
      "|        Roht|      Deborah|1951-11-16|\n",
      "|    Schuster|       Albert|1981-12-21|\n",
      "|       Sauer| Hans-Günther|1984-08-11|\n",
      "|    Eckbauer|       Donata|1945-04-30|\n",
      "|       Stoll|       Cosima|1991-08-12|\n",
      "|    Rosemann|     Arnfried|1971-03-30|\n",
      "|      Hering|   Kreszentia|1990-12-03|\n",
      "|    Barkholz|       Herwig|1996-06-04|\n",
      "|        Beer|     Giuseppe|1979-08-04|\n",
      "|      Becker|  Horst-Peter|1954-02-05|\n",
      "|    Mitschke|        Klara|1989-01-24|\n",
      "|        Dörr|     Birgitta|1988-10-02|\n",
      "|       Löwer|         Katy|1982-12-02|\n",
      "|    Lachmann|         Leni|1987-06-11|\n",
      "|     Weitzel|     Stefania|1963-07-08|\n",
      "|       Ebert|  Anne-Katrin|1990-02-19|\n",
      "|  Möchlichen| Klaus-Jürgen|1971-11-30|\n",
      "|    Röhricht|         Elfi|1968-05-08|\n",
      "|        Stey|    Mechthild|2004-03-25|\n",
      "|      Kramer|        Ahmed|1958-03-02|\n",
      "|       Plath|        Kläre|1969-04-10|\n",
      "|     Harloff|Hans-Heinrich|1956-08-12|\n",
      "|      Scholl|        Zoran|1979-03-16|\n",
      "| Grein Groth|          Ole|2002-02-05|\n",
      "|       Linke|       Magnus|1944-10-11|\n",
      "|       Wirth|        Samir|1992-05-17|\n",
      "|        Wiek|       Gülsen|1980-04-29|\n",
      "|        Ring|   Hans Peter|1978-07-21|\n",
      "|      Berger|        Halil|1959-08-11|\n",
      "|        Salz|  Hans-Detlef|1951-07-16|\n",
      "|      Dowerg|         Elke|2000-12-03|\n",
      "|      Johann|      Sibylla|2005-09-28|\n",
      "|      Ehlert|  Hans-Albert|2005-05-13|\n",
      "|        Weiß|    Karl-Otto|1996-06-18|\n",
      "|      Etzler|      Guntram|1946-09-22|\n",
      "|      Etzler|        Helga|1944-05-14|\n",
      "|Tschentscher|    Christina|1960-09-19|\n",
      "|     Stiffel|      Florian|2003-07-10|\n",
      "|      Döring|       Ilonka|1958-11-10|\n",
      "|        Holt|         Gisa|1951-10-01|\n",
      "|       Krebs|        Erkan|1979-03-03|\n",
      "|     Schüler|      Aurelia|1981-12-13|\n",
      "|       Römer|  Hans-Günter|2004-10-06|\n",
      "|     Ruppert|        Paula|1995-12-29|\n",
      "|        Kaul|     Ingeborg|1996-05-19|\n",
      "|      Täsche|        Gitta|2005-11-04|\n",
      "+------------+-------------+----------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading from PostgreSQL\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Databases\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark = SparkSession \\\n",
    "#    .builder \\\n",
    "#    .appName(\"Databases\") \\\n",
    "# location of the drivers in non-Docker envs\n",
    "# in Docker driver must be copied into container to /usr/local/spark/jars with docker cp\n",
    "#    .config(\"spark.jars\", \"/home/peter/projects/spark/postgresql-42.7.0.jar\") \\ \n",
    "#    .getOrCreate()\n",
    "\n",
    "def show_customers(spark: SparkSession, database) -> None:\n",
    "    df_customers = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://172.17.0.2:5432/postgres\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .option(\"dbtable\", \"customers\") \\\n",
    "        .option(\"user\", \"postgres\") \\\n",
    "        .option(\"password\", \"guiltyspark\") \\\n",
    "        .load()\n",
    "    df_customers.select('last_name', 'first_name', 'birth_date').show(100)\n",
    "\n",
    "show_customers(spark, \"postgresql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b849b4c5-04bc-43f2-9d52-731b1edd4b6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/peter/projects/onboarding/notebooks/home/peter/projects/onboarding/data/csv/states.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome/peter/projects/onboarding/data/csv/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m parquet_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome/peter/projects/onboarding/data/parquet/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m states_csv_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msep\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m|\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstates.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m states_csv_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(parquet_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m states_parquet_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(parquet_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter-spark/lib/python3.10/site-packages/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter-spark/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter-spark/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/peter/projects/onboarding/notebooks/home/peter/projects/onboarding/data/csv/states.csv."
     ]
    }
   ],
   "source": [
    "# Read a CSV file and convert it to Parquet\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSV-to-Parquet converter\").getOrCreate()\n",
    "\n",
    "# For Docker\n",
    "# csv_path = \"work/data/csv/\"\n",
    "# parquet_path = \"work/data/parquet/\"\n",
    "\n",
    "# For Conda\n",
    "csv_path = \"/home/peter/projects/onboarding/data/csv/\"\n",
    "parquet_path = \"/home/peter/projects/onboarding/data/parquet/\"\n",
    "\n",
    "\n",
    "states_csv_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \"|\").load(csv_path + \"states.csv\")\n",
    "states_csv_df.write.mode(\"overwrite\").parquet(parquet_path + \"states.parquet\")\n",
    "states_parquet_df = spark.read.parquet(parquet_path + \"states.parquet\")\n",
    "states_parquet_df.createOrReplaceTempView(\"states\")\n",
    "states_sql_df = spark.sql(\"SELECT * FROM states\")\n",
    "states_sql_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
